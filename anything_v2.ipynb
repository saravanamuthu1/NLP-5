{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "IH_TlH3wc3a3"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "import helper\n",
        "import numpy as np\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "from keras.models import Model\n",
        "from keras.layers import GRU, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional\n",
        "from keras.layers import Embedding\n",
        "from keras.optimizers import Adam\n",
        "from keras.losses import sparse_categorical_crossentropy\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install keras-self-attention"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZMX6Xm7Bdek",
        "outputId": "f4ee3149-9290-4e43-de0e-22b7ead1c937"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: keras-self-attention in /usr/local/lib/python3.7/dist-packages (0.51.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras-self-attention) (1.21.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Bidirectional, Conv1D, MaxPool1D, Dense, Flatten, Dropout, AveragePooling2D, LSTM, TimeDistributed, Attention\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "import keras\n",
        "from keras import models, layers\n",
        "from keras import backend\n",
        "from sklearn.metrics import f1_score,recall_score,precision_score, confusion_matrix\n",
        "import keras\n",
        "from keras_self_attention import SeqSelfAttention\n",
        "import collections\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "from keras.models import Model\n",
        "from keras.layers import GRU, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional\n",
        "from keras.layers import Embedding\n",
        "from keras.optimizers import Adam\n",
        "from keras.losses import sparse_categorical_crossentropy\n",
        "from keras.models import Sequential"
      ],
      "metadata": {
        "id": "B1eduBjkBRJw"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ARq3Hlw6c3a7"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/SLU Semesters/SLU 3rd Semester/NLP/Fifth Competition/\n",
        "\n",
        "%ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "xVjkxNcKc3a8"
      },
      "outputs": [],
      "source": [
        "# def read_data(file_name):\n",
        "#   data=[]\n",
        "#   with open(file_name, encoding='utf8') as f:\n",
        "#     for line in f:\n",
        "#       line=line.strip()\n",
        "#       data.append(line)\n",
        "#   size = len(data)\n",
        "#   idx_list = [idx + 1 for idx, val in\n",
        "#             enumerate(data) if val == '</s>']\n",
        "#   res = [data[i: j] for i, j in\n",
        "#         zip([0] + idx_list, idx_list + \n",
        "#         ([size] if idx_list[-1] != size else []))]\n",
        "#   return res"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def read_data(file_name):\n",
        "    all_data = []\n",
        "    descript = 'Reading ' + file_name\n",
        "\n",
        "    f = open(file_name, 'r', encoding='utf-8')\n",
        "    full_text = f.read()\n",
        "\n",
        "    cur_sent = []\n",
        "\n",
        "    for line in tqdm(full_text.split('\\n'), desc=descript):\n",
        "        if line == '<s>':\n",
        "            cur_sent = []\n",
        "            continue\n",
        "        if line in '()':\n",
        "            continue\n",
        "        if line == '</s>':\n",
        "            all_data.append(cur_sent)\n",
        "            continue\n",
        "        else:\n",
        "            cur_sent.append(line.lower())\n",
        "\n",
        "    return all_data"
      ],
      "metadata": {
        "id": "G37itZVo-zRi"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9v8T6Dtitl4",
        "outputId": "0d1ab6b2-adec-4918-b084-d5dca69582f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mmodels\u001b[0m/  Neural_Machine_Translation.ipynb  \u001b[01;34mtest-05\u001b[0m/  \u001b[01;34mtrain-05\u001b[0m/\n"
          ]
        }
      ],
      "source": [
        "%ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SBdkrEiAfhuY"
      },
      "outputs": [],
      "source": [
        "# source = read_data('train-05/train-source.txt')\n",
        "# target = read_data('train-05/train-target.txt')\n",
        "\n",
        "source = read_data('test-05/test-source.txt')\n",
        "target = read_data('test-05/test-target.txt')\n",
        "\n",
        "source"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "jA4UyvNtc3a8"
      },
      "outputs": [],
      "source": [
        "# source: https://towardsdatascience.com/neural-machine-translation-with-python-c2f0a34f7dd\n",
        "\n",
        "def tokenize(x):\n",
        "    x_tk = Tokenizer(char_level = False)\n",
        "    x_tk.fit_on_texts(x)\n",
        "    return x_tk.texts_to_sequences(x), x_tk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "di7wYtWac3a9"
      },
      "outputs": [],
      "source": [
        "# source: https://towardsdatascience.com/neural-machine-translation-with-python-c2f0a34f7dd\n",
        "\n",
        "def pad(x, length=None):\n",
        "    if length is None:\n",
        "        length = max([len(sentence) for sentence in x])\n",
        "    return pad_sequences(x, maxlen = length, padding = 'post')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "gxVc8XYoc3a9"
      },
      "outputs": [],
      "source": [
        "# source: https://towardsdatascience.com/neural-machine-translation-with-python-c2f0a34f7dd\n",
        "\n",
        "def preprocess(x, y):\n",
        "    preprocess_x, x_tk = tokenize(x)\n",
        "    preprocess_y, y_tk = tokenize(y)\n",
        "\n",
        "    preprocess_x = pad(preprocess_x)\n",
        "    preprocess_y = pad(preprocess_y)\n",
        "\n",
        "    # Keras's sparse_categorical_crossentropy function requires the labels to be in 3 dimensions\n",
        "    preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n",
        "\n",
        "    return preprocess_x, preprocess_y, x_tk, y_tk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Zeho1L-kkVBc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65612584-2637-4fe8-dac5-4690d8d27a7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Preprocessed\n",
            "Max source sentence length: 97\n",
            "Max target sentence length: 92\n",
            "Source vocabulary size: 3215\n",
            "Target vocabulary size: 2997\n"
          ]
        }
      ],
      "source": [
        "# source: https://towardsdatascience.com/neural-machine-translation-with-python-c2f0a34f7dd\n",
        "\n",
        "preproc_source, preproc_target, source_tokenizer, target_tokenizer =\\\n",
        "    preprocess(source, target)\n",
        "    \n",
        "max_source_length = preproc_source.shape[1]\n",
        "max_target_length = preproc_target.shape[1]\n",
        "source_vocab_size = len(source_tokenizer.word_index)\n",
        "target_vocab_size = len(target_tokenizer.word_index)\n",
        "\n",
        "print('Data Preprocessed')\n",
        "print(\"Max source sentence length:\", max_source_length)\n",
        "print(\"Max target sentence length:\", max_target_length)\n",
        "print(\"Source vocabulary size:\", source_vocab_size)\n",
        "print(\"Target vocabulary size:\", target_vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "wQgg4qnvc3a-"
      },
      "outputs": [],
      "source": [
        "# source: https://towardsdatascience.com/neural-machine-translation-with-python-c2f0a34f7dd\n",
        "\n",
        "def logits_to_text(logits, tokenizer):\n",
        "    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
        "    index_to_words[0] = ''\n",
        "\n",
        "    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "DM8uiovjc3a_"
      },
      "outputs": [],
      "source": [
        "# source: https://towardsdatascience.com/neural-machine-translation-with-python-c2f0a34f7dd\n",
        "\n",
        "def simple_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
        "    learning_rate = 1e-3\n",
        "    input_seq = Input(input_shape[1:])\n",
        "    rnn = GRU(64, return_sequences = True)(input_seq)\n",
        "    logits = TimeDistributed(Dense(french_vocab_size))(rnn)\n",
        "    model = Model(input_seq, Activation('softmax')(logits))\n",
        "    model.compile(loss = sparse_categorical_crossentropy, \n",
        "                 optimizer = Adam(learning_rate), \n",
        "                 metrics = ['accuracy'])\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def build_model(input_shape, output_sequence_length, source_vocab_size, target_vocab_size):\n",
        "\n",
        "#     model = Sequential()\n",
        "#     model.add(Embedding(input_dim=source_vocab_size,output_dim=256,input_length=input_shape[1]))\n",
        "#     model.add(Bidirectional(LSTM(256, activation=\"tanh\",return_sequences=True)))\n",
        "#     model.add(SeqSelfAttention(attention_activation='softmax'))\n",
        "#     # model.add((LSTM(512, activation = 'relu')))\n",
        "#     model.add(Dense(512, activation = 'relu'))\n",
        "#     model.add(Dense(128, activation = 'relu'))\n",
        "#     model.add(Dense(64, activation = 'relu'))\n",
        "#     model.add((Dense(target_vocab_size,activation='softmax')))\n",
        "#     learning_rate = 0.005\n",
        "\n",
        "#     model.compile(loss = sparse_categorical_crossentropy,\n",
        "#                  optimizer = Adam(learning_rate),\n",
        "#                  metrics = ['accuracy'])\n",
        "\n",
        "#     return model"
      ],
      "metadata": {
        "id": "LrGA1E07AoKS"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tmp_x = pad(preproc_source, max_target_length)\n",
        "tmp_x = tmp_x.reshape((-1, preproc_target.shape[-2], 1))"
      ],
      "metadata": {
        "id": "LVBzJguC2IEu"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the neural network\n",
        "simple_rnn_model = simple_model(\n",
        "    tmp_x.shape,\n",
        "    max_target_length,\n",
        "    source_vocab_size,\n",
        "    target_vocab_size)\n",
        "\n",
        "simple_rnn_model.fit(tmp_x, preproc_target, batch_size=32, epochs=1, validation_split=0.2)\n",
        "\n",
        "# Print prediction(s)\n",
        "print(logits_to_text(simple_rnn_model.predict(tmp_x[:1])[0], target_tokenizer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKQ6rQHQCFgn",
        "outputId": "7b5b4d01-ad7b-406d-f7b0-acbccef3e3f5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1130/1130 [==============================] - 169s 145ms/step - loss: 0.9777 - accuracy: 0.9195 - val_loss: nan - val_accuracy: 0.9287\n",
            "1/1 [==============================] - 0s 333ms/step\n",
            "\" a a a a a a a a a a a . .                                                                                                                                                                                                               \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "mG5gTtaXc3a_"
      },
      "outputs": [],
      "source": [
        "# # source: https://towardsdatascience.com/neural-machine-translation-with-python-c2f0a34f7dd\n",
        "\n",
        "# # Train RNN\n",
        "\n",
        "# # Train the neural network\n",
        "# simple_rnn_model = simple_model(\n",
        "#     tmp_x.shape,\n",
        "#     max_target_length,\n",
        "#     source_vocab_size,\n",
        "#     target_vocab_size)\n",
        "\n",
        "# simple_rnn_model.fit(tmp_x, preproc_target, batch_size=32, epochs=1, validation_split=0.2)\n",
        "\n",
        "# # Print prediction(s)\n",
        "# print(logits_to_text(simple_rnn_model.predict(tmp_x[:1])[0], target_tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "simple_rnn_model.save('./models/simple_rnn_model.h5')"
      ],
      "metadata": {
        "id": "YCklcCKVxrLz"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%ls ./models/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VptNN4zTyUJp",
        "outputId": "930ef641-3d94-4692-ca5e-c295a18ba7b1"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "simple_rnn_model.h5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "loaded_rnn = load_model(\"./models/simple_rnn_model.h5\")"
      ],
      "metadata": {
        "id": "k3mmg2ijyA2X"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Train RNN for test\n",
        "\n",
        "# preproc_source_test, preproc_target_test, source_test_tokenizer, target_test_tokenizer =\\\n",
        "#     preprocess(source_test, target_test)\n",
        "    \n",
        "# max_source_test_length = preproc_source_test.shape[1]\n",
        "# max_target_test_length = preproc_target_test.shape[1]\n",
        "# source_test_vocab_size = len(source_test_tokenizer.word_index)\n",
        "# target_test_vocab_size = len(target_test_tokenizer.word_index)\n",
        "\n",
        "# tmp_x_test = pad(preproc_source_test, max_target_test_length)\n",
        "# tmp_x_test = tmp_x_test.reshape((-1, preproc_target_test.shape[-2], 1))"
      ],
      "metadata": {
        "id": "dX-WOb9rzFon"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_rnn.predict(tmp_x[:1])[0]"
      ],
      "metadata": {
        "id": "yL-eT4381ZNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(logits_to_text(loaded_rnn.predict(tmp_x[:1])[0], target_tokenizer))"
      ],
      "metadata": {
        "id": "thkfG5WBz7ox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "NUScC_Vbc3bA"
      },
      "outputs": [],
      "source": [
        "# source: https://towardsdatascience.com/neural-machine-translation-with-python-c2f0a34f7dd\n",
        "from keras.models import Sequential\n",
        "\n",
        "def embed_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
        "    learning_rate = 1e-3\n",
        "    rnn = GRU(64, return_sequences=True, activation=\"tanh\")\n",
        "    \n",
        "    embedding = Embedding(french_vocab_size, 64, input_length=input_shape[1]) \n",
        "    logits = TimeDistributed(Dense(french_vocab_size, activation=\"softmax\"))\n",
        "    \n",
        "    model = Sequential()\n",
        "    #em can only be used in first layer --> Keras Documentation\n",
        "    model.add(embedding)\n",
        "    model.add(rnn)\n",
        "    model.add(logits)\n",
        "    model.compile(loss=sparse_categorical_crossentropy,\n",
        "                  optimizer=Adam(learning_rate),\n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train embedding model\n",
        "\n",
        "tmp_x = pad(preproc_source, max_target_length)\n",
        "tmp_x = tmp_x.reshape((-1, preproc_target.shape[-2]))\n",
        "\n",
        "\n",
        "embeded_model = embed_model(\n",
        "    tmp_x.shape,\n",
        "    max_target_length,\n",
        "    source_vocab_size,\n",
        "    target_vocab_size)\n",
        "\n",
        "embeded_model.fit(tmp_x, preproc_target, batch_size=32, epochs=1, validation_split=0.2)\n",
        "\n",
        "print(logits_to_text(embeded_model.predict(tmp_x[:1])[0], target_tokenizer))"
      ],
      "metadata": {
        "id": "r-EJ5H_Dx8No"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "-2WnA_2Wc3bB"
      },
      "outputs": [],
      "source": [
        "# source: https://towardsdatascience.com/neural-machine-translation-with-python-c2f0a34f7dd\n",
        "\n",
        "def bd_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
        "    learning_rate = 1e-3\n",
        "    model = Sequential()\n",
        "    model.add(Bidirectional(GRU(128, return_sequences = True, dropout = 0.1), \n",
        "                           input_shape = input_shape[1:]))\n",
        "    model.add(TimeDistributed(Dense(french_vocab_size, activation = 'softmax')))\n",
        "    model.compile(loss = sparse_categorical_crossentropy, \n",
        "                 optimizer = Adam(learning_rate), \n",
        "                 metrics = ['accuracy'])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "49xi-SWCc3bB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7146ba08-616a-404e-945c-f84931dea5d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "1130/1130 [==============================] - 341s 294ms/step - loss: 0.7411 - accuracy: 0.9194 - val_loss: 0.4874 - val_accuracy: 0.9287\n",
            "Epoch 2/2\n",
            "1130/1130 [==============================] - 336s 297ms/step - loss: 0.5464 - accuracy: 0.9204 - val_loss: 0.4844 - val_accuracy: 0.9290\n",
            "1/1 [==============================] - 1s 589ms/step\n",
            "bhí an a a a a a a a a a a . .                                                                                                                                                                                                               \n"
          ]
        }
      ],
      "source": [
        "# source: https://towardsdatascience.com/neural-machine-translation-with-python-c2f0a34f7dd\n",
        "# Train bi-directional\n",
        "tmp_x = pad(preproc_source, preproc_target.shape[1])\n",
        "tmp_x = tmp_x.reshape((-1, preproc_target.shape[-2], 1))\n",
        "\n",
        "bidi_model = bd_model(\n",
        "    tmp_x.shape,\n",
        "    preproc_target.shape[1],\n",
        "    len(source_tokenizer.word_index)+1,\n",
        "    len(target_tokenizer.word_index)+1)\n",
        "\n",
        "\n",
        "bidi_model.fit(tmp_x, preproc_target, batch_size=32, epochs=2, validation_split=0.2)\n",
        "\n",
        "# Print prediction(s)\n",
        "print(logits_to_text(bidi_model.predict(tmp_x[:1])[0], target_tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bidi_model.save('./models/bidire_model.h5')\n",
        "from keras.models import load_model\n",
        "loaded_bid = load_model(\"./models/bidire_model.h5\")"
      ],
      "metadata": {
        "id": "7P9Rkg0TSuV-"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(logits_to_text(loaded_bid.predict(tmp_x[:1])[0], target_tokenizer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZbgIYB40TVC9",
        "outputId": "837ff5f2-9f7a-40a5-9682-51055b4e4180"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 556ms/step\n",
            "bhí an . . . . . . . . . . a a                                                                                                                                                                                                               \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FI1Kmh7gc3bB"
      },
      "outputs": [],
      "source": [
        "# source: https://towardsdatascience.com/neural-machine-translation-with-python-c2f0a34f7dd\n",
        "\n",
        "def encdec_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
        "    learning_rate = 1e-3\n",
        "    model = Sequential()\n",
        "    model.add(GRU(128, input_shape = input_shape[1:], return_sequences = False))\n",
        "    model.add(RepeatVector(output_sequence_length))\n",
        "    model.add(GRU(128, return_sequences = True))\n",
        "    model.add(TimeDistributed(Dense(french_vocab_size, activation = 'softmax')))\n",
        "    \n",
        "    model.compile(loss = sparse_categorical_crossentropy, \n",
        "                 optimizer = Adam(learning_rate), \n",
        "                 metrics = ['accuracy'])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hMZzTgT4c3bB"
      },
      "outputs": [],
      "source": [
        "\n",
        "# source: https://towardsdatascience.com/neural-machine-translation-with-python-c2f0a34f7dd\n",
        "\n",
        "def model_final(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=english_vocab_size,output_dim=128,input_length=input_shape[1]))\n",
        "    model.add(Bidirectional(GRU(256,return_sequences=False)))\n",
        "    model.add(RepeatVector(output_sequence_length))\n",
        "    model.add(Bidirectional(GRU(256,return_sequences=True)))\n",
        "    model.add(TimeDistributed(Dense(french_vocab_size,activation='softmax')))\n",
        "    learning_rate = 0.005\n",
        "    \n",
        "    model.compile(loss = sparse_categorical_crossentropy, \n",
        "                 optimizer = Adam(learning_rate), \n",
        "                 metrics = ['accuracy'])\n",
        "    \n",
        "    return model"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.7 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}