{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "IH_TlH3wc3a3"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "import helper\n",
        "import numpy as np\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "from keras.models import Model\n",
        "from keras.layers import GRU, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional\n",
        "from keras.layers import Embedding\n",
        "from keras.optimizers import Adam\n",
        "from keras.losses import sparse_categorical_crossentropy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARq3Hlw6c3a7",
        "outputId": "3f3ea364-e0b3-47e2-eb04-23582cb19089"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/SLU Semesters/SLU 3rd Semester/NLP/Fifth Competition\n",
            "'Neural Machine Translation.ipynb'   \u001b[0m\u001b[01;34mtest-05\u001b[0m/   \u001b[01;34mtrain-05\u001b[0m/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/SLU Semesters/SLU 3rd Semester/NLP/Fifth Competition/\n",
        "\n",
        "%ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "xVjkxNcKc3a8"
      },
      "outputs": [],
      "source": [
        "# source: https://towardsdatascience.com/neural-machine-translation-with-python-c2f0a34f7dd\n",
        "\n",
        "def read_data(file_name):\n",
        "  data=[]\n",
        "  with open(file_name, encoding='utf8') as f:\n",
        "    for line in f:\n",
        "      line=line.strip()\n",
        "      data.append(line)\n",
        "  size = len(data)\n",
        "  idx_list = [idx + 1 for idx, val in\n",
        "            enumerate(data) if val == '</s>']\n",
        "  res = [data[i: j] for i, j in\n",
        "        zip([0] + idx_list, idx_list + \n",
        "        ([size] if idx_list[-1] != size else []))]\n",
        "  return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9v8T6Dtitl4",
        "outputId": "dc37b4ab-3570-411a-d013-070afd992920"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'Neural Machine Translation.ipynb'   \u001b[0m\u001b[01;34mtest-05\u001b[0m/   \u001b[01;34mtrain-05\u001b[0m/\n"
          ]
        }
      ],
      "source": [
        "%ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SBdkrEiAfhuY"
      },
      "outputs": [],
      "source": [
        "source = read_data('train-05//train-source.txt')\n",
        "target = read_data('train-05/train-target.txt')\n",
        "source"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "jA4UyvNtc3a8"
      },
      "outputs": [],
      "source": [
        "# source: https://towardsdatascience.com/neural-machine-translation-with-python-c2f0a34f7dd\n",
        "\n",
        "def tokenize(x):\n",
        "    x_tk = Tokenizer(char_level = False)\n",
        "    x_tk.fit_on_texts(x)\n",
        "    return x_tk.texts_to_sequences(x), x_tk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "di7wYtWac3a9"
      },
      "outputs": [],
      "source": [
        "# source: https://towardsdatascience.com/neural-machine-translation-with-python-c2f0a34f7dd\n",
        "\n",
        "def pad(x, length=None):\n",
        "    if length is None:\n",
        "        length = max([len(sentence) for sentence in x])\n",
        "    return pad_sequences(x, maxlen = length, padding = 'post')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "gxVc8XYoc3a9"
      },
      "outputs": [],
      "source": [
        "# source: https://towardsdatascience.com/neural-machine-translation-with-python-c2f0a34f7dd\n",
        "\n",
        "def preprocess(x, y):\n",
        "    preprocess_x, x_tk = tokenize(x)\n",
        "    preprocess_y, y_tk = tokenize(y)\n",
        "\n",
        "    preprocess_x = pad(preprocess_x)\n",
        "    preprocess_y = pad(preprocess_y)\n",
        "\n",
        "    # Keras's sparse_categorical_crossentropy function requires the labels to be in 3 dimensions\n",
        "    preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n",
        "\n",
        "    return preprocess_x, preprocess_y, x_tk, y_tk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zeho1L-kkVBc"
      },
      "outputs": [],
      "source": [
        "# source: https://towardsdatascience.com/neural-machine-translation-with-python-c2f0a34f7dd\n",
        "\n",
        "preproc_source, preproc_target, source_tokenizer, target_tokenizer =\\\n",
        "    preprocess(source, target)\n",
        "    \n",
        "max_source_length = preproc_source.shape[1]\n",
        "max_target_length = preproc_target.shape[1]\n",
        "source_vocab_size = len(source_tokenizer.word_index)\n",
        "target_vocab_size = len(target_tokenizer.word_index)\n",
        "\n",
        "print('Data Preprocessed')\n",
        "print(\"Max source sentence length:\", max_source_length)\n",
        "print(\"Max target sentence length:\", max_target_length)\n",
        "print(\"Source vocabulary size:\", source_vocab_size)\n",
        "print(\"Target vocabulary size:\", target_vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "wQgg4qnvc3a-"
      },
      "outputs": [],
      "source": [
        "# source: https://towardsdatascience.com/neural-machine-translation-with-python-c2f0a34f7dd\n",
        "\n",
        "def logits_to_text(logits, tokenizer):\n",
        "    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
        "    index_to_words[0] = ''\n",
        "\n",
        "    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "DM8uiovjc3a_"
      },
      "outputs": [],
      "source": [
        "# source: https://towardsdatascience.com/neural-machine-translation-with-python-c2f0a34f7dd\n",
        "\n",
        "def simple_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
        "    learning_rate = 1e-3\n",
        "    input_seq = Input(input_shape[1:])\n",
        "    rnn = GRU(64, return_sequences = True)(input_seq)\n",
        "    logits = TimeDistributed(Dense(french_vocab_size))(rnn)\n",
        "    model = Model(input_seq, Activation('softmax')(logits))\n",
        "    model.compile(loss = sparse_categorical_crossentropy, \n",
        "                 optimizer = Adam(learning_rate), \n",
        "                 metrics = ['accuracy'])\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mG5gTtaXc3a_",
        "outputId": "4e15b8e9-838a-478d-abfc-3c51e146cc24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            " 242/1130 [=====>........................] - ETA: 1:09:58 - loss: 2.4616 - accuracy: 0.9128"
          ]
        }
      ],
      "source": [
        "# source: https://towardsdatascience.com/neural-machine-translation-with-python-c2f0a34f7dd\n",
        "\n",
        "# Train RNN\n",
        "tmp_x = pad(preproc_source, max_target_length)\n",
        "tmp_x = tmp_x.reshape((-1, preproc_target.shape[-2], 1))\n",
        "\n",
        "# Train the neural network\n",
        "simple_rnn_model = simple_model(\n",
        "    tmp_x.shape,\n",
        "    max_target_length,\n",
        "    source_vocab_size,\n",
        "    target_vocab_size)\n",
        "simple_rnn_model.fit(tmp_x, preproc_target, batch_size=32, epochs=10, validation_split=0.2)\n",
        "\n",
        "# Print prediction(s)\n",
        "print(logits_to_text(simple_rnn_model.predict(tmp_x[:1])[0], target_tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NUScC_Vbc3bA"
      },
      "outputs": [],
      "source": [
        "# source: https://towardsdatascience.com/neural-machine-translation-with-python-c2f0a34f7dd\n",
        "\n",
        "def embed_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
        "    learning_rate = 1e-3\n",
        "    rnn = GRU(64, return_sequences=True, activation=\"tanh\")\n",
        "    \n",
        "    embedding = Embedding(french_vocab_size, 64, input_length=input_shape[1]) \n",
        "    logits = TimeDistributed(Dense(french_vocab_size, activation=\"softmax\"))\n",
        "    \n",
        "    model = Sequential()\n",
        "    #em can only be used in first layer --> Keras Documentation\n",
        "    model.add(embedding)\n",
        "    model.add(rnn)\n",
        "    model.add(logits)\n",
        "    model.compile(loss=sparse_categorical_crossentropy,\n",
        "                  optimizer=Adam(learning_rate),\n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-2WnA_2Wc3bB"
      },
      "outputs": [],
      "source": [
        "# source: https://towardsdatascience.com/neural-machine-translation-with-python-c2f0a34f7dd\n",
        "\n",
        "def bd_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
        "    learning_rate = 1e-3\n",
        "    model = Sequential()\n",
        "    model.add(Bidirectional(GRU(128, return_sequences = True, dropout = 0.1), \n",
        "                           input_shape = input_shape[1:]))\n",
        "    model.add(TimeDistributed(Dense(french_vocab_size, activation = 'softmax')))\n",
        "    model.compile(loss = sparse_categorical_crossentropy, \n",
        "                 optimizer = Adam(learning_rate), \n",
        "                 metrics = ['accuracy'])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49xi-SWCc3bB"
      },
      "outputs": [],
      "source": [
        "# source: https://towardsdatascience.com/neural-machine-translation-with-python-c2f0a34f7dd\n",
        "\n",
        "tmp_x = pad(preproc_english_sentences, preproc_french_sentences.shape[1])\n",
        "tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2], 1))\n",
        "bidi_model = bd_model(\n",
        "    tmp_x.shape,\n",
        "    preproc_french_sentences.shape[1],\n",
        "    len(english_tokenizer.word_index)+1,\n",
        "    len(french_tokenizer.word_index)+1)\n",
        "bidi_model.fit(tmp_x, preproc_french_sentences, batch_size=1024, epochs=20, validation_split=0.2)\n",
        "# Print prediction(s)\n",
        "print(logits_to_text(bidi_model.predict(tmp_x[:1])[0], french_tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FI1Kmh7gc3bB"
      },
      "outputs": [],
      "source": [
        "# source: https://towardsdatascience.com/neural-machine-translation-with-python-c2f0a34f7dd\n",
        "\n",
        "def encdec_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
        "    learning_rate = 1e-3\n",
        "    model = Sequential()\n",
        "    model.add(GRU(128, input_shape = input_shape[1:], return_sequences = False))\n",
        "    model.add(RepeatVector(output_sequence_length))\n",
        "    model.add(GRU(128, return_sequences = True))\n",
        "    model.add(TimeDistributed(Dense(french_vocab_size, activation = 'softmax')))\n",
        "    \n",
        "    model.compile(loss = sparse_categorical_crossentropy, \n",
        "                 optimizer = Adam(learning_rate), \n",
        "                 metrics = ['accuracy'])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hMZzTgT4c3bB"
      },
      "outputs": [],
      "source": [
        "\n",
        "# source: https://towardsdatascience.com/neural-machine-translation-with-python-c2f0a34f7dd\n",
        "\n",
        "def model_final(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=english_vocab_size,output_dim=128,input_length=input_shape[1]))\n",
        "    model.add(Bidirectional(GRU(256,return_sequences=False)))\n",
        "    model.add(RepeatVector(output_sequence_length))\n",
        "    model.add(Bidirectional(GRU(256,return_sequences=True)))\n",
        "    model.add(TimeDistributed(Dense(french_vocab_size,activation='softmax')))\n",
        "    learning_rate = 0.005\n",
        "    \n",
        "    model.compile(loss = sparse_categorical_crossentropy, \n",
        "                 optimizer = Adam(learning_rate), \n",
        "                 metrics = ['accuracy'])\n",
        "    \n",
        "    return model"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.7 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.7"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
